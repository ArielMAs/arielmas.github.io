<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Automating Job Data Collection</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.css" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <style>
        body { padding: 20px; }
        pre { background: #f8f9fa; padding: 10px; border-radius: 5px; }
    </style>
</head>
<body>
    <div class="container">
        <h1 class="mb-4">Automating Job Market Data Collection: API Calls, Data Cleaning, and Storage</h1>
        <h2>Introduction</h2>
        <p>
            In today’s dynamic job market, staying on top of employment trends is essential for both job seekers and organizations.
            However, accessing and analyzing this vast amount of data can be time-consuming and complex.
            That’s why I developed a tool to streamline this process by leveraging data from the USAJobs API.
        </p>
        <p>
            In this blog, I'll walk through my Job Market Gov project, a solution designed to collect, process,
            and store job listing data from the <a href='https://developer.usajobs.gov/'>USAJobs API</a>.
            The project extracts key information from job listings such as position titles, locations, and salary data,
            storing it in efficient Parquet files for further analysis.
        </p>
        <h2>Project Overview</h2>
        <p>The USAJobs API provides public job listings from U.S. government agencies.
        </p>
        <p>
            <b>The project aims to:</b>
            <li>Fetch job listings based on a set of parameters like date posted, location, and position.</li>
            <li>Extract valuable insights from these listings, such as position titles, location details, and remuneration data.
            <li>Store the data in Parquet files for easy querying and further analysis.</li>
        <h2>How It Works</h2>
        <p>
        <b>The project is designed in three main parts:</b>
        <ol>
        <li><b>API Data Fetching & Processing</b>: A shell script (run.sh) manages API requests, fetching job listings from the USAJobs API.
            The script handles pagination, ensuring we don’t hit the API rate limits while collecting up to 10,000 rows of data.</li>
        <li><b>Data Processing in Python:</b> Once the data is fetched,
            a Python script (job_listing.py) uses Pandas to clean and process the data:</li>
            <ul>
            <li>It extracts relevant columns like job position, location, and salary.</li>
            <li>Nested JSON objects, such as job duties and location details, are flattened into simpler, more accessible formats.</li>
            <li>Salary data is normalized based on the rate interval (e.g., hourly or annual), allowing for easy comparison across jobs.</li>
            </ul>
        <li><b>Storing Data</b>: The processed data is stored in Parquet format. Parquet is a columnar storage format, optimized for reading large datasets and ideal for analytical processing. The project writes the processed data into jobs, duties, locations, and job_category Parquet files.
        </ol>
        <h2>Why This Approach?</h2>
        <li><b>Scalability:</b> The USAJobs API provides a large amount of data. By breaking the data into pages and using efficient file formats like Parquet, we can handle datasets with thousands of records without compromising performance.</li>
        <li><b>Flexibility:</b> The data is stored in Parquet files, which are easy to query and analyze with data analysis tools like Pandas or even business intelligence platforms like Tableau.</li>
        <li><b>Automation:</b> The process is automated using Docker, ensuring that the environment is consistently replicated and that the tool can be easily deployed or updated.</li>

        <h2>Key Features of Job Market Gov</h2>
        <li><b>Pagination Handling:</b> The tool handles API rate limits and pagination, ensuring that data is retrieved efficiently without hitting the API's limits.
        <li><b>Data Normalization:</b> Nested JSON structures are flattened using Pandas’ json_normalize, making the data easier to work with.
        <li><b>Remuneration Calculation:</b> The tool normalizes salary data, converting hourly wages to an annual salary when possible. This helps to compare job positions more easily.

        <h2> Step one: API Fetching</h2>
        <p>
        We use run.sh to handle API requests, authentication, and pagination automatically.
        </p>
        <b>This script:</b>
            <li>Authenticates using an <a href="https://developer.usajobs.gov/apirequest/">Email and API key</a></li>
            <li>Fetches job listings in pages of 500 results for the last 60 days</li>
            <li>Handles pagination until all jobs are retrieved</li>
            <li>Extracts key job details using jq (JSON processor)</li>
            <li>Stores data in main.json for processing</li>
        </ul>
        <!-- Collapsible Sections -->
        <div class="accordion" id="jobDataAccordion">
            <div class="accordion-item">
                <h2 class="accordion-header">
                    <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#apiCall">Step 1: Fetching Job Listings</button>
                </h2>
                <div id="apiCall" class="accordion-collapse collapse show">
                    <div class="accordion-body">
                        <pre><code class="language-bash">
#!/bin/bash

#Search Jobs API rate limitations
#Maximum of 10,000 rows per query
#Maximum of 500 rows per page

EMAIL=$API_EMAIL
AUTH_KEY=$API_AUTH
BASE_URL="https://data.usajobs.gov/api/search"
ROWS_PER_PAGE=500
MAX_ROWS=10000
PAGE=1

echo running
echo '[]' > main.json

# Loop through pages until no more results
echo "Connection to API started"
while true; do
    # Make the request for the current page
    curl -H "Host: data.usajobs.gov" \
            -H "User-Agent: $EMAIL" \
            -H "Authorization-Key: $AUTH_KEY" \
            "$BASE_URL?page=$PAGE&ResultsPerPage=$ROWS_PER_PAGE&DatePosted=60" \
            -o "page_$PAGE.json"
    
    #Get number of results from current page .json
    RESULT_COUNT=$(jq '.SearchResult.SearchResultCount' "page_$PAGE.json")

    #Select specific objects
    jq '[.SearchResult.SearchResultItems[].MatchedObjectDescriptor 
        | {PositionID, PositionTitle, PositionLocationDisplay, PositionLocation, OrganizationName, DepartmentName, JobCategory, QualificationSummary, PositionRemuneration, PublicationStartDate, UserArea: {Details: {JobSummary: .UserArea.Details.JobSummary, MajorDuties: .UserArea.Details.MajorDuties}}}]' \
        "page_$PAGE.json" > "tmp.json" && mv tmp.json "page_$PAGE.json"

    # Merge the current page data into main.json
    jq -s 'add' main.json "page_$PAGE.json" > "tmp.json" && mv tmp.json main.json
    
    # Remove current page
    rm page_$PAGE.json
    # Break if result has less than 500 rows
    if [ "$RESULT_COUNT" -lt $ROWS_PER_PAGE ]; then
        break
    fi

    echo "Page: "$PAGE
    echo "Result Count: "$RESULT_COUNT
    # Increment page number
    PAGE=$((PAGE + 1))
done

echo "Connection to API ended. Long JSON is ready."
</code></pre>
                    </div>
                </div>
            </div>
            <br>
            <h2>Step 2: Structuring the Data</h2>
            <p>
            Once we have the raw data, we need to clean and process it,
            particularly salary normalization,
            since salaries are reported in different formats (hourly, annually, etc.).
            </p>
            <p>
            <b>This script:</b>
            <li>Loads JSON data extracted in step 1
            <li>Normalizes salaries to a yearly format (hourly wages converted)
            <li>Computes salary ranges (min, max, mean, range)
            <li>Stores processed data in Parquet format for efficient querying
            </ul>
            <div class="accordion-item">
                <h2 class="accordion-header">
                    <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#dataCleaning">Step 2: Cleaning Data</button>
                </h2>
                <div id="dataCleaning" class="accordion-collapse collapse">
                    <div class="accordion-body">
                        <pre><code class="language-python">
import pandas as pd
import argparse
import os


def sub_object_json_extractor(df,json_col,columns_to_keep):
    loce_df = df[['PositionID',json_col]].explode(json_col)#this wil be moved to its own table

    # Flatten the nested dictionary in the 'PositionLocation' column
    location_df = pd.json_normalize(loce_df[json_col])
    # Merge back into the original dataframe
    locations = loce_df.join(location_df[columns_to_keep]).drop(json_col,axis=1)
    return locations

def rumenation_extractor(df):
    # Flatten the nested dictionary in the 'PositionLocation' column
    remuneration_df_explode = df.explode('PositionRemuneration')
    remuneration_df = pd.json_normalize(remuneration_df_explode['PositionRemuneration'])

    # Select only the required columns
    columns_to_keep = ['MinimumRange', 'MaximumRange','RateIntervalCode']

    # Merge back into the original dataframe
    df = df.join(remuneration_df[columns_to_keep])

    df['MinimumRange'] = pd.to_numeric(df['MinimumRange'])
    df['MaximumRange'] = pd.to_numeric(df['MaximumRange'])
    #feature generating
    #Yearly Salary=Hourly Wage×40×52
    def normalized_salary(row,col_name):
        s = row[col_name]
        rateCode = row['RateIntervalCode']
        if rateCode=='PH':
            return 40*52*s
        elif rateCode=='PA':
            return s

    df['NormalizedMinSalary'] = df.apply(lambda row: normalized_salary(row, 'MinimumRange'), axis=1)
    df['NormalizedMaxSalary'] = df.apply(lambda row: normalized_salary(row, 'MaximumRange'), axis=1)
    df['NormalizedMeanSalary'] = df[["NormalizedMinSalary", "NormalizedMaxSalary"]].mean(axis=1)
    df['NormalizedRangeSalary'] = df['NormalizedMaxSalary']-df['NormalizedMinSalary']
    return df


def write_to_parquet(jobs,duties,locations,job_category,first_run):
    os.makedirs('./job-market-gov/data/', exist_ok=True)
    # Path to the Parquet file
    files_dict = {'jobs':jobs,
                    'duties':duties,
                    'locations':locations,
                    'job_category':job_category}
    for df_name,df_current in files_dict.items():
        p_path = f'./job-market-gov/data/{df_name}.parquet'
        if first_run:
            # Write the DataFrame back to the Parquet file
            df_current.to_parquet(p_path, engine='pyarrow', index=False)
        else:
            try:
                # Try reading the existing Parquet file
                existing_df = pd.read_parquet(p_path)
                # Append the new data to the existing DataFrame
                combined_df = pd.concat([existing_df, df_current], ignore_index=True)
            except FileNotFoundError:
                # If the file doesn't exist, create a new DataFrame with the new data
                combined_df = df_current

            # Write the combined DataFrame back to the Parquet file
            combined_df.to_parquet(p_path, engine='pyarrow', index=False)


def main(path,first_run):
    print('starting')
    df_clean = pd.read_json(path)
    userArea = pd.json_normalize(df_clean['UserArea'],sep='_')
    df_clean = df_clean.join(userArea)
    locations = sub_object_json_extractor(df=df_clean,json_col='PositionLocation',columns_to_keep=['LocationName', 'CountryCode', 'CountrySubDivisionCode', 'CityName', 'Longitude', 'Latitude'])
    job_category = sub_object_json_extractor(df=df_clean,json_col='JobCategory',columns_to_keep=['Name', 'Code'])
    df_clean = rumenation_extractor(df=df_clean)
    df_clean['date_col'] = pd.to_datetime(df_clean['PublicationStartDate']).dt.date
    duties = df_clean[['PositionID','Details_MajorDuties']].explode('Details_MajorDuties')
    jobs = df_clean.drop(['UserArea','PositionLocation','JobCategory','Details_MajorDuties','MinimumRange','MaximumRange','RateIntervalCode','PublicationStartDate','PositionRemuneration'],axis=1)
    write_to_parquet(jobs,duties,locations,job_category,first_run)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Process job market data and store in parquet files.")
    parser.add_argument("path", type=str, help="Path to the JSON file")
    parser.add_argument("--first_run",
                        type=bool,
                        default=False,  # Default value if not provided
                        help="Indicate if it's the first run or not (default is False)")
    
    args = parser.parse_args()
    # Access the value of first_run
    main(args.path,args.first_run)
                            
</code></pre>
                    </div>
                </div>
            </div>
            <br>
            <h2>Step 3: Dockerizing the Project</h2>
            <p>
                To ensure the tool runs in a consistent, reproducible environment, we use Docker.
                This removes issues with dependencies and makes the setup process effortless.
            </p>

            <div class="accordion-item">
                <h2 class="accordion-header">
                    <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#dataStorage">Step 3: Storing Data</button>
                </h2>
                <div id="dataStorage" class="accordion-collapse collapse">
                    <div class="accordion-body">
                        <pre><code class="language-python">
FROM ubuntu:latest

COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

WORKDIR /app

RUN apt-get update && apt-get install -y curl \
                                            jq \
                                            python3.12 \
                                            && rm -rf /var/lib/apt/lists/*


# Copy the project into the image
COPY pyproject.toml run.sh uv.lock job-market-gov/job_listing.py .

# Define build arguments
ARG API_EMAIL
ARG API_AUTH

# Set environment variables from build arguments
ENV API_EMAIL=${API_EMAIL}
ENV API_AUTH=${API_AUTH}

# Give execute permissions to the shell script
RUN chmod +x ./run.sh && ./run.sh

# Create a virtual environment and install dependencies
RUN uv venv && \
    . .venv/bin/activate && \
    uv sync --frozen && \
    python3 job_listing.py main.json --first_run True
        </code></pre>
                    </div>
                </div>
            </div>
        </div>
        <br>
            <h2>Conclusion</h2>
            <p>
            The Job Market Gov tool automates the collection and analysis of government job listings.
            By handling pagination, salary normalization, and structured data storage,
            it provides a highly efficient dataset for job market analysis.
            </p>

            <h2>Key Takeaways</h2>
            <ol>
                <li> Automated API fetching handles pagination and large data retrieval
                <li> Salary normalization ensures consistent salary comparison
                <li> Parquet storage makes querying faster and more efficient
                <li> Dockerization simplifies deployment and makes it easily reproducible
            </ol>

            <h2>Next Steps</h2>
            <ol>
                <li>Explore & Clean data</li>
                <li>Build a dashboard for visualization</li>
                <li>Perform data analysis on job trends</li>
                <li>Automate updates every X days using GitHub Actions</li>
            </ol>
            <p>For the full source code, Parquet files, and instructions on running the pipeline,
        check out the <a href="https://github.com/ArielMAs/arielmas.github.io/tree/main/src/job-market-gov" target="_blank">full project</a> on GitHub!
        </p>
    </div>
</body>
</html>